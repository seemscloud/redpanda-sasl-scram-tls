apiVersion: v1
kind: ConfigMap
metadata:
  name: consumer
  labels:
    app.kubernetes.io/name: consumer
data:
  consumer.py: |
    #!/usr/bin/env python3
    import os
    import signal
    import sys
    import time
    from datetime import datetime, timezone
    from confluent_kafka import Consumer, KafkaError

    STOP = False


    def log(message):
      ts = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")
      print(f"{ts} [consumer] {message}", flush=True)


    def on_term(_signum, _frame):
      global STOP
      STOP = True
      log("Received termination signal. Exiting.")


    def sleep_interruptible(seconds):
      end = time.time() + seconds
      while not STOP and time.time() < end:
        time.sleep(min(0.2, end - time.time()))


    signal.signal(signal.SIGINT, on_term)
    signal.signal(signal.SIGTERM, on_term)

    TOPIC = os.getenv("TOPIC", "logs")
    BROKER_SERVICE = os.getenv("BROKER_SERVICE", "redpanda")
    BROKER_PORT = os.getenv("BROKER_PORT", "9093")
    NAMESPACE = os.getenv("NAMESPACE", "default")
    BROKER = f"{BROKER_SERVICE}.{NAMESPACE}.svc.cluster.local.:{BROKER_PORT}"

    KAFKA_USERNAME = os.getenv("KAFKA_USERNAME", "consumer")
    KAFKA_PASSWORD_FILE = os.getenv("KAFKA_PASSWORD_FILE", "/etc/redpanda/auth/password")
    CA_FILE = os.getenv("CA_FILE", "/etc/redpanda/certs/ca.crt")
    GROUP_ID = os.getenv("GROUP_ID", "logs-consumer")
    AUTO_OFFSET_RESET = os.getenv("AUTO_OFFSET_RESET", "earliest")

    try:
      POLL_TIMEOUT_MS = int(os.getenv("POLL_TIMEOUT_MS", "1000"))
    except ValueError:
      log("POLL_TIMEOUT_MS must be an integer.")
      sys.exit(1)

    try:
      MAX_RECORDS_PER_POLL = int(os.getenv("MAX_RECORDS_PER_POLL", "100"))
    except ValueError:
      log("MAX_RECORDS_PER_POLL must be an integer.")
      sys.exit(1)

    try:
      MIN_MESSAGES_PER_BATCH = int(os.getenv("MIN_MESSAGES_PER_BATCH", "100"))
    except ValueError:
      log("MIN_MESSAGES_PER_BATCH must be an integer.")
      sys.exit(1)

    try:
      MAX_BACKOFF_SECONDS = int(os.getenv("MAX_BACKOFF_SECONDS", "30"))
    except ValueError:
      log("MAX_BACKOFF_SECONDS must be an integer.")
      sys.exit(1)

    if POLL_TIMEOUT_MS < 1:
      log("POLL_TIMEOUT_MS must be >= 1")
      sys.exit(1)

    if MAX_RECORDS_PER_POLL < 1:
      log("MAX_RECORDS_PER_POLL must be >= 1")
      sys.exit(1)

    if MIN_MESSAGES_PER_BATCH < 1:
      log("MIN_MESSAGES_PER_BATCH must be >= 1")
      sys.exit(1)

    POLL_TIMEOUT_SECONDS = POLL_TIMEOUT_MS / 1000.0


    def wait_for_dependencies():
      while not STOP:
        password_ready = os.path.isfile(KAFKA_PASSWORD_FILE) and os.path.getsize(KAFKA_PASSWORD_FILE) > 0
        ca_ready = os.path.isfile(CA_FILE) and os.path.getsize(CA_FILE) > 0
        if password_ready and ca_ready:
          return True
        log(f"Waiting for mounted secrets ({KAFKA_PASSWORD_FILE}, {CA_FILE})...")
        sleep_interruptible(2)
      return False


    def read_password():
      with open(KAFKA_PASSWORD_FILE, "r", encoding="utf-8") as password_file:
        return password_file.read().strip()


    def create_consumer():
      password = read_password()
      consumer = Consumer({
        "bootstrap.servers": BROKER,
        "security.protocol": "SASL_SSL",
        "sasl.mechanism": "SCRAM-SHA-512",
        "sasl.username": KAFKA_USERNAME,
        "sasl.password": password,
        "ssl.ca.location": CA_FILE,
        "group.id": GROUP_ID,
        "auto.offset.reset": AUTO_OFFSET_RESET,
        "enable.auto.commit": True,
      })
      consumer.subscribe([TOPIC])
      return consumer


    def can_access_topic(consumer):
      metadata = consumer.list_topics(topic=TOPIC, timeout=10)
      topic_metadata = metadata.topics.get(TOPIC)
      if topic_metadata is not None and topic_metadata.error is not None:
        raise RuntimeError(f"Topic metadata error for {TOPIC}: {topic_metadata.error}")
      return True


    def collect_batch(consumer):
      batch_count = 0
      while not STOP and batch_count < MIN_MESSAGES_PER_BATCH:
        to_fetch = min(MAX_RECORDS_PER_POLL, MIN_MESSAGES_PER_BATCH - batch_count)
        messages = consumer.consume(num_messages=to_fetch, timeout=POLL_TIMEOUT_SECONDS)
        if not messages:
          continue

        for message in messages:
          if message is None:
            continue

          err = message.error()
          if err is None:
            batch_count += 1
            continue

          if err.code() == KafkaError._PARTITION_EOF:
            continue

          raise RuntimeError(f"Kafka consume error: {err}")

      return batch_count


    def main():
      if not wait_for_dependencies():
        return 0

      log(
        f"Consumer configured. topic={TOPIC} broker={BROKER} "
        f"user={KAFKA_USERNAME} group={GROUP_ID} min_batch={MIN_MESSAGES_PER_BATCH}"
      )

      backoff = 1
      total = 0

      while not STOP:
        consumer = None
        try:
          consumer = create_consumer()
          if not can_access_topic(consumer):
            raise RuntimeError(f"Cannot access topic metadata: {TOPIC}")
          backoff = 1
          log("Connected to Redpanda. Starting consumer stream.")

          while not STOP:
            consumed = collect_batch(consumer)
            if consumed == 0:
              continue
            total += consumed
            log(f"Consumed {consumed} messages (batch_min={MIN_MESSAGES_PER_BATCH}, total={total})")
        except Exception as err:
          if STOP:
            break
          log(f"Consumer stream interrupted. Retrying in {backoff}s. Reason: {err}")
          sleep_interruptible(backoff)
          backoff = min(backoff * 2, MAX_BACKOFF_SECONDS)
        finally:
          if consumer is not None:
            try:
              consumer.close()
            except Exception:
              pass

      return 0


    if __name__ == "__main__":
      try:
        sys.exit(main())
      except Exception as err:
        log(f"Fatal error: {err}")
        sys.exit(1)
